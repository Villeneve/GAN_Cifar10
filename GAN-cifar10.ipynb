{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.api as keras\n",
    "from keras.api.layers import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(_,_) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train[y_train[:,0] == 8]\n",
    "x_train = (tf.cast(x_train,tf.float32)-127.5) / 127.5\n",
    "\n",
    "n = np.random.randint(0,x_train.shape[0])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((x_train)).shuffle(x_train.shape[0]).batch(batch_size,drop_remainder=True)\n",
    "\n",
    "complete_hist = {\n",
    "    'loss_dis': [],\n",
    "    'loss_gen': [],\n",
    "}\n",
    "\n",
    "plt.imshow(x_train[n])\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class miniBatch(keras.layers.Layer):\n",
    "    def __init__(self,num_kernels,kernel_dim,batch_size):\n",
    "        super(miniBatch,self).__init__()\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        self.T = self.add_weight(\n",
    "            shape=(input_shape[-1],self.num_kernels*self.kernel_dim), # Teoricamente 128x500\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        M = tf.matmul(x,self.T) # teoricamente 128x128 \\times 128x500 = 128x500\n",
    "        M = tf.reshape(M,(-1,self.num_kernels,self.kernel_dim)) # teoricamente 128x100x5\n",
    "        M_T = tf.expand_dims(M,1) # teoricamente 128x1x100x5\n",
    "        M = tf.expand_dims(M,0) # teoricamente 1x128x100x5\n",
    "        diff = tf.abs(M-M_T)\n",
    "        exp_diff = tf.exp(-tf.reduce_mean(diff,-1))\n",
    "        miniBatch_features = tf.reduce_sum(exp_diff,1)\n",
    "        output = tf.concat([x,miniBatch_features],-1)\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Define a forma de saída explicitamente\n",
    "        return (input_shape[0], input_shape[1] + self.num_kernels)\n",
    "\n",
    "class PixelShuffle(keras.layers.Layer):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(PixelShuffle, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        h = input_shape[1]\n",
    "        w = input_shape[2]\n",
    "        c = input_shape[3]\n",
    "        out_c = c // (self.upscale_factor ** 2)\n",
    "        x = tf.reshape(inputs, (batch_size, h, w, self.upscale_factor, self.upscale_factor, out_c))\n",
    "        x = tf.transpose(x, [0, 1, 2, 4, 3, 5])\n",
    "        x = tf.reshape(x, (batch_size, h * self.upscale_factor, w * self.upscale_factor, out_c))\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        h, w, c = input_shape[1], input_shape[2], input_shape[3]\n",
    "        out_c = c // (self.upscale_factor ** 2)\n",
    "        return (input_shape[0], h * self.upscale_factor, w * self.upscale_factor, out_c)\n",
    "    \n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, filters):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.query_conv = Conv2D(filters // 8, kernel_size=1)\n",
    "        self.key_conv = Conv2D(filters // 8, kernel_size=1)\n",
    "        self.value_conv = Conv2D(filters, kernel_size=1)\n",
    "        self.softmax = Softmax(axis=-1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        batch, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "\n",
    "        Q = tf.reshape(self.query_conv(x), (batch, height * width, -1))  # [B, HW, C/8]\n",
    "        K = tf.reshape(self.key_conv(x), (batch, -1, height * width))    # [B, C/8, HW]\n",
    "        V = tf.reshape(self.value_conv(x), (batch, height * width, -1))  # [B, HW, C]\n",
    "\n",
    "        attention_map = self.softmax(tf.matmul(Q, K))  # [B, HW, HW]\n",
    "\n",
    "        attention_output = tf.matmul(attention_map, V)  # [B, HW, C]\n",
    "        attention_output = tf.reshape(attention_output, (batch, height, width, channels))\n",
    "\n",
    "        return attention_output + x\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def residual_Gblock(x):\n",
    "\n",
    "    skip = x\n",
    "    x = Conv2D(x.shape[-1],3,1,'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(.2)(x)\n",
    "    x = Conv2D(x.shape[-1],3,1,'same')(x)\n",
    "    x = Add()([x,skip])\n",
    "\n",
    "    return x\n",
    "\n",
    "def residual_Dblock(x):\n",
    "\n",
    "    skip = x\n",
    "    x = Conv2D(x.shape[-1],3,1,'same')(x)\n",
    "    x = LeakyReLU(.2)(x)\n",
    "    x = Conv2D(x.shape[-1],3,1,'same')(x)\n",
    "    x = Add()([x,skip])\n",
    "    x = LeakyReLU(.2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_discriminator():\n",
    "    \n",
    "    input = Input((32,32,3))\n",
    "    x = input\n",
    "    k = 1\n",
    "    for _ in range(4):\n",
    "        x = Conv2D(512//k,3,2,'same')(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        k = k*2\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = miniBatch(100,5,128)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "\n",
    "    return keras.Model(input,x,name='Discriminator')\n",
    "\n",
    "def create_generator():\n",
    "\n",
    "    input = Input((128,))\n",
    "\n",
    "    x = Dense(8*8*2048)(input)\n",
    "    x = LeakyReLU(.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Reshape((8,8,2048))(x)\n",
    "    x = SelfAttention(x.shape[-1])(x)\n",
    "\n",
    "    for _ in range(2):\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2DTranspose(256,3,1,'same')(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2DTranspose(3,3,1,'same')(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    return keras.Model(input,x)\n",
    "\n",
    "gen = create_generator()\n",
    "dis = create_discriminator()\n",
    "dis.summary()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = keras.losses.BinaryCrossentropy()\n",
    "MSE = keras.losses.MeanSquaredError()\n",
    "\n",
    "LR = 1e-4\n",
    "\n",
    "gen_opt = keras.optimizers.Adam(LR,.5)\n",
    "dis_opt = keras.optimizers.Adam(LR/2,.5)\n",
    "n = 5\n",
    "noise_out = tf.random.normal((n**2,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(dis.layers):\n",
    "    print(f'L = {i}, layer = {layer}')\n",
    "\n",
    "#n = np.random.uniform(0,1,(1,32,32,3))\n",
    "\n",
    "maps = keras.Model(dis.input, dis.layers[7].output)\n",
    "# map = maps(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_weights(model,clip_value=0.1):\n",
    "    \n",
    "    for layer in model.trainable_variables:\n",
    "        layer.assign(tf.clip_by_value(layer, -clip_value, clip_value))\n",
    "\n",
    "# def gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "#     alpha = tf.random.uniform((real_samples.shape[0],), 0, 1)\n",
    "#     interpolated_samples = alpha[0] * real_samples + (1 - alpha[0]) * fake_samples\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         tape.watch(interpolated_samples)\n",
    "#         predictions = discriminator(interpolated_samples)\n",
    "#     gradients = tape.gradient(predictions, interpolated_samples)\n",
    "#     gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n",
    "#     penalty = tf.reduce_mean((gradients_norm - 1) ** 2)\n",
    "#     return penalty\n",
    "\n",
    "# def wasser_dist(y_true,y_pred):\n",
    "#     return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    gen_loss,dis_loss = 0.,0.\n",
    "    gen_loss_iter,dis_loss_iter = 0.,0.\n",
    "    for batch in data:\n",
    "        \n",
    "        noise = tf.random.normal((batch_size,128))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "            fake_imgs = gen(noise,training=True)\n",
    "            true_labels = dis(batch,training=True)\n",
    "            fake_labels = dis(fake_imgs,training=True)\n",
    "            #true_map = maps(batch,training=True)\n",
    "            #fake_map = maps(fake_imgs, training=True)\n",
    "\n",
    "            \n",
    "            gen_loss_iter = bce(tf.ones_like(fake_labels),fake_labels) #- (1/2)*tf.reduce_mean(tf.math.reduce_std(fake_imgs,axis=0))\n",
    "            dis_loss_iter = bce(tf.ones_like(true_labels),true_labels) + bce(tf.zeros_like(fake_labels),fake_labels)\n",
    "            # dis_loss_iter = tf.reduce_mean(fake_labels) - tf.reduce_mean(true_labels)\n",
    "            # gen_loss_iter = -tf.reduce_mean(fake_labels) + MSE(true_map,fake_map)\n",
    "        \n",
    "        gen_gras = gen_tape.gradient(gen_loss_iter,gen.trainable_variables)\n",
    "        gen_opt.apply_gradients(zip(gen_gras,gen.trainable_variables))\n",
    "\n",
    "        dis_grads = dis_tape.gradient(dis_loss_iter,dis.trainable_variables)\n",
    "        dis_opt.apply_gradients(zip(dis_grads,dis.trainable_variables))\n",
    "\n",
    "        gen_loss += gen_loss_iter/batch_size\n",
    "        dis_loss += dis_loss_iter/batch_size\n",
    "        gen_loss_iter,dis_loss_iter = 0.,0.\n",
    "\n",
    "    return gen_loss/tf.cast(len(data),tf.float32),dis_loss/tf.cast(len(data),tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "EPOCH_SAMPLE = 10\n",
    "\n",
    "loss_dis, loss_gen = 0.,0.\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    # Histórico de Loss\n",
    "    \n",
    "    loss_gen, loss_dis = train_step()\n",
    "    #loss_gen, loss_dis = loss_gen.numpy(), loss_dis.numpy()\n",
    "    clip_weights(dis,0.1)\n",
    "    complete_hist['loss_gen'].append(loss_gen)\n",
    "    complete_hist['loss_dis'].append(loss_dis)\n",
    "    \n",
    "    # Iteração das épocas\n",
    "    if i % EPOCH_SAMPLE == 0:\n",
    "        # Print Loss\n",
    "        print(f'Ep = {i} | Loss_gen = {loss_gen:.4f}; Loss_dis = {loss_dis:.4f}')\n",
    "        # Salvar uma amostra das imagens\n",
    "        img_fake = gen(noise_out)\n",
    "        fig, ax = plt.subplots(n,n,figsize=(1,1))\n",
    "        ax = ax.ravel()\n",
    "        for ii in range(n**2):\n",
    "            ax[ii].imshow(np.uint8(img_fake[ii]*127.5+127.5))\n",
    "            ax[ii].set_axis_off()\n",
    "        plt.savefig(f'imgs/fig{i}.png',dpi=1000)\n",
    "        plt.close()\n",
    "\n",
    "    plt.semilogy(np.array(complete_hist['loss_gen']),label=f'GEN = {loss_gen:.4f}')\n",
    "    plt.semilogy(np.array(complete_hist['loss_dis']),label=f'DIS = {loss_dis:.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True,'minor')\n",
    "    plt.savefig('loss.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print('==================== COMPLETE ====================')\n",
    "\n",
    "# plt.Figure()\n",
    "# plt.plot(loss_list[:,0],label='Discriminator')\n",
    "# plt.plot(loss_list[:,1],label='GAN')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.save('generator_cars.keras')\n",
    "dis.save('discriminator_cars.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "noise_out = tf.random.normal((1000,1024))\n",
    "\n",
    "img_fake = gen(noise_out)\n",
    "out_true = dis(x_train[0:1000])\n",
    "out_fake = dis(gen(tf.random.uniform((1000,1024))))\n",
    "\n",
    "print(f'Média True = {tf.reduce_mean(out_true):.4f} \\nMédia False = {tf.reduce_mean(out_fake):.4f}')\n",
    "\n",
    "plt.Figure()\n",
    "plt.semilogy(np.array(complete_hist['loss_gen']),label='GEN')\n",
    "plt.semilogy(np.array(complete_hist['loss_dis']),label='DIS')\n",
    "plt.legend()\n",
    "plt.grid(True,'minor')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(n,n,figsize=(10,10))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(n**2):\n",
    "    ax[i].imshow(img_fake[i])\n",
    "    ax[i].set_axis_off()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
